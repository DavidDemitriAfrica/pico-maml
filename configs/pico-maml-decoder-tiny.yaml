# pico-maml-decoder-tiny.yaml

checkpointing:
  run_name: "pico-maml-decoder-tiny-1"
  save_to_hf: true
  hf_checkpoint:
    repo_id: "davidafrica/pico-maml-decoder-tiny"
  save_every_n_steps: 1000
  learning_dynamics:
    batch_size: 256

model:
  # tiny decoder
  d_model: 96
  activation_hidden_dim: 384
  n_layers: 4
  n_heads: 4
  dropout: 0.1

monitoring:
  save_to_wandb: true
  wandb:
    project: "pico-maml"
    entity: "pico-lm"

training:
  max_steps: 20000
  optimization:
    gradient_accumulation_steps: 4

  fabric:
    precision: "bf16"
    num_nodes: 4
    num_devices: 4

smlmt:
  enabled: true
  hybrid_ratio: 0.5        # fraction of batches to do MAML
  inner_steps: 10           # K inner‐loop SGD steps
  inner_lr: 0.01           # inner‐loop learning rate
  min_token_freq: 1        # only mask tokens ≥ this freq
  max_token_freq: -1       # no upper‐bound
  classifier_head:
    num_layers: 2
    hidden_dim: 128
    dropout: 0.1
    init_method: "xavier"

data:
  dataset:
    name: "pico-lm/pretokenized-dolma"
    split: "train"
    cache_dir: "/home/dda28/rds/hpc-work/shared_hf_cache/pico-lm/pretokenized-dolma"
  dataloader:
    batch_size: 64
    num_workers: 4

evaluation:
  paloma:
    batch_size: 32
