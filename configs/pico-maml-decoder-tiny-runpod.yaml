# runpod-pico-maml-decoder-tiny.yaml

checkpointing:
  # still needs a run_name for logs, but we won’t save anything
  run_name: "pico-maml-decoder-tiny-runpod-7"
  save_to_hf: false         # no pushes to HuggingFace
  auto_resume: false        # don’t try to resume from disk
  save_every_n_steps: 0     # never checkpoint
  learning_dynamics:
    layer_suffixes: []      # disable LD collection entirely

model:
  # tiny decoder
  d_model: 24
  activation_hidden_dim: 96
  max_seq_len: 512 

monitoring:
  save_to_wandb: false      # no WandB file logs (stdout only)
  logging:
    log_every_n_steps: 10  # you can still print to console

training:
  max_steps: 20000
  optimization:
    gradient_accumulation_steps: 4
    gradient_checkpointing: true

  fabric:
    precision: "bf16"
    devices: 4              # use 4 GPUs on this node
    num_nodes: 1

smlmt:
  enabled: false
  hybrid_ratio: 0.5
  inner_steps: 10
  inner_lr: 0.01
  min_token_freq: 1
  max_token_freq: -1
  classifier_head:
    num_layers: 2
    hidden_dim: 128
    dropout: 0.1
    init_method: "xavier"

data:
  dataset:
    name: "pico-lm/pretokenized-dolma"
    split: "train"
    # no cache_dir so HF will stream into ephemeral storage
  dataloader:
    batch_size: 8         # 256 is the default, but we can override it here

evaluation:
  paloma:
    batch_size: 32
