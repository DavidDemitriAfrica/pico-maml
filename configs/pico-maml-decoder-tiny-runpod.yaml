# runpod-pico-maml-decoder-tiny.yaml

checkpointing:
  # still needs a run_name for logs, but we won’t save anything
  run_name: "pico-maml-decoder-tiny-runpod-20"
  save_to_hf: false         # no pushes to HuggingFace
  auto_resume: false        # don’t try to resume from disk
  save_every_n_steps: 100     # never checkpoint
  learning_dynamics:
    layer_suffixes: []      # disable LD collection entirely

model:
  # tiny decoder
  d_model: 96
  activation_hidden_dim: 384
  max_seq_len: 512 

monitoring:
  save_to_wandb: true      # no WandB file logs (stdout only)
  logging:
    log_every_n_steps: 10  # you can still print to console

training:
  max_steps: 20000
  optimization:
    gradient_accumulation_steps: 4
    gradient_checkpointing: true

  fabric:
    precision: "bf16-mixed"
    devices: 4              # use 4 GPUs on this node
    num_nodes: 1

smlmt:
  enabled: true
  hybrid_ratio: 0.5
  inner_steps: 20
  inner_lr: 0.01
  min_token_freq: 10
  max_token_freq: 10000
  classifier_head:
    num_layers: 4
    hidden_dim: 128
    dropout: 0.1
    init_method: "xavier"
  support_size: 10

data:
  dataset:
    name: "pico-lm/pretokenized-dolma"
    split: "train"
    # no cache_dir so HF will stream into ephemeral storage
  dataloader:
    batch_size: 256         # 256 is the default, but we can override it here

evaluation:
  paloma:
    batch_size: 32
