# Demo config file 
# You can follow this template to create your own config file
# Refer to the config files in the configs/ directory to see all the available options

data:
  dataloader:
    batch_size: 16
  
checkpointing:
  run_name: "inner_loop_100_80_2_4"
  save_every_n_steps: 100

  save_checkpoint_repo_id: "davidafrica/pico-maml"

  learning_dynamics:
    batch_size: 8

model:
    d_model: 96
    activation_hidden_dim: 384

evaluation:
  metrics:
    - universal_ner
  universal_ner:
    dataset_name: "universalner/universal_ner"
    dataset_config: "en_pud"  # Choose one from: ['ceb_gja', 'zh_gsd', 'zh_gsdsimp', 'zh_pud', ...]
    dataset_split: "test"
    batch_size: 16

monitoring:
  logging:
    log_every_n_steps: 10

training:
  max_steps: 1000

  optimization:
    lr: 0.001
    lr_warmup_steps: 30

    gradient_accumulation_steps: 4
  
  fabric:
    num_nodes: 1
    num_devices: 4

smlmt:
  enabled: true
  probability: 1        # probability that the meta–learning component is computed on any given iteration
  num_classes: 2           # K: number of target words per task e.g [2, 5, 10]
  support_per_class: 80     # S: number of support sentences per class e.g [5, 10, 20]
  query_per_class: 10       # Q: number of query sentences per class
  smlmt_weight: 1  # weight to scale the meta–learning loss relative to the supervised loss
  inner_lr: 1e-1
  inner_steps: 100 # e.g [20, 50, 100]