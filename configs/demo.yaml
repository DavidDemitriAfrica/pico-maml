# Demo config file 
# You can follow this template to create your own config file
# Refer to the config files in the configs/ directory to see all the available options

data:
  dataloader:
    batch_size: 32
  
checkpointing:
  run_name: "test-13"
  save_every_n_steps: 50

  save_checkpoint_repo_id: "davidafrica/pico-maml"

  learning_dynamics:
    batch_size: 16

model:
    d_model: 96
    activation_hidden_dim: 384

evaluation: 
  paloma:
    batch_size: 32

monitoring:
  logging:
    log_every_n_steps: 10

training:
  max_steps: 100

  optimization:
    lr: 0.001
    lr_warmup_steps: 30

    gradient_accumulation_steps: 2
  
  fabric:
    num_devices: 1

smlmt:
  enabled: true
  probability: 0.5        # probability that the meta–learning component is computed on any given iteration
  num_classes: 3           # K: number of target words per task
  support_per_class: 2     # S: number of support sentences per class
  query_per_class: 2       # Q: number of query sentences per class
  smlmt_weight: 1  # weight to scale the meta–learning loss relative to the supervised loss
  inner_lr: 0.001
  inner_steps: 1