# Demo config file 
# You can follow this template to create your own config file
# Refer to the config files in the configs/ directory to see all the available options

data:
  dataloader:
    batch_size: 16
  
checkpointing:
  run_name: "multigpu-test-52"
  save_every_n_steps: 100

  save_checkpoint_repo_id: "davidafrica/pico-maml"

  learning_dynamics:
    batch_size: 8

model:
    d_model: 96
    activation_hidden_dim: 394

evaluation:
  metrics:
    - universal_ner
  universal_ner:
    dataset_name: "universalner/universal_ner"
    dataset_config: "en_pud"  # Choose one from: ['ceb_gja', 'zh_gsd', 'zh_gsdsimp', 'zh_pud', ...]
    dataset_split: "test"
    batch_size: 16

monitoring:
  logging:
    log_every_n_steps: 10

training:
  max_steps: 10000

  optimization:
    lr: 0.001
    lr_warmup_steps: 30

    gradient_accumulation_steps: 1
  
  fabric:
    num_nodes: 1
    num_devices: 2

smlmt:
  enabled: true
  probability: 0.2        # probability that the meta–learning component is computed on any given iteration
  num_classes: 3           # K: number of target words per task
  support_per_class: 3     # S: number of support sentences per class
  query_per_class: 3       # Q: number of query sentences per class
  smlmt_weight: 1  # weight to scale the meta–learning loss relative to the supervised loss
  inner_lr: 0.001
  inner_steps: 5